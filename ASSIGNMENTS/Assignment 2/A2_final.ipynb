{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#from transformers import Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 - CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data():\n",
    "    \n",
    "    #encodng the data into pandas.DataFrame objects\n",
    "    url_a_test = 'Data/arguments-test.tsv'\n",
    "    df_a_test = pd.read_csv(url_a_test, sep='\\t')\n",
    "\n",
    "    url_a_training = 'Data/arguments-training.tsv'\n",
    "    df_a_training = pd.read_csv(url_a_training, sep='\\t')\n",
    "\n",
    "    url_a_validation = 'Data/arguments-validation.tsv'\n",
    "    df_a_validation = pd.read_csv(url_a_validation, sep='\\t')\n",
    "\n",
    "    url_l_test = 'Data/labels-test.tsv'\n",
    "    df_l_test = pd.read_csv(url_l_test, sep='\\t')\n",
    "\n",
    "    url_l_training = 'Data/labels-training.tsv'\n",
    "    df_l_training = pd.read_csv(url_l_training, sep='\\t')\n",
    "\n",
    "    url_l_validation = 'Data/labels-validation.tsv'\n",
    "    df_l_validation = pd.read_csv(url_l_validation, sep='\\t')\n",
    "\n",
    "    #merge argument dataframes with label dataframes\n",
    "    df_test = pd.merge(df_a_test, df_l_test, on='Argument ID')\n",
    "    df_training = pd.merge(df_a_training, df_l_training, on='Argument ID')\n",
    "    df_validation = pd.merge(df_a_validation, df_l_validation, on='Argument ID')\n",
    "\n",
    "    return df_test, df_training, df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, df_training, df_validation = load_and_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_drop_columns(df):\n",
    "    # Merge level 2 annotations to level 3 categories\n",
    "    df['Openess to change'] = df[['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism']].any(axis=1).astype(int)\n",
    "    df['Self-enhancement'] = df[['Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face']].any(axis=1).astype(int)\n",
    "    df['Conservation'] = df[['Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility']].any(axis=1).astype(int)\n",
    "    df['Self-transcendence'] = df[['Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity']].any(axis=1).astype(int)\n",
    "    \n",
    "    # Drop unuseful columns\n",
    "    columns_to_drop = ['Argument ID', 'Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_test = merge_and_drop_columns(df_test)\n",
    "df_training = merge_and_drop_columns(df_training)\n",
    "df_validation = merge_and_drop_columns(df_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION\n",
    "Ancora da inserire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding 'Stance' column into numerical format  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training['Stance'] = df_training['Stance'].replace({'in favor of': 1, 'against': 0}).astype(int)\n",
    "df_validation['Stance'] = df_validation['Stance'].replace({'in favor of': 1, 'against': 0}).astype(int)\n",
    "df_test['Stance'] = df_test['Stance'].replace({'in favor of': 1, 'against': 0}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for tokenization input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = df_test.iloc[:, 3:7].values\n",
    "labels_training = df_training.iloc[:, 3:7].values\n",
    "labels_validation = df_validation.iloc[:, 3:7].values\n",
    "\n",
    "stance_test = df_test['Stance'].values\n",
    "stance_training = df_training['Stance'].values\n",
    "stance_validation = df_validation['Stance'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "ds_training = Dataset.from_pandas(df_training)\n",
    "ds_validation = Dataset.from_pandas(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "max_len = 100\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenization(ds, stance, model_type):\n",
    "    # Initialize lists to store the results\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    " \n",
    "    if model_type == 'c':\n",
    "        c_texts = ds['Conclusion']\n",
    "        \n",
    "        for text in c_texts:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "            \n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "\n",
    "        # Combine the results into a dictionary\n",
    "        df_c_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        return df_c_inputs\n",
    "    \n",
    "    elif model_type == 'cp':\n",
    "        # Extract the list of texts for tokenization of BERT_cp and BERT_cps model inputs\n",
    "        cp_texts = ds['Conclusion']+[\" \"]+ds['Premise']\n",
    "\n",
    "        for text in cp_texts:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        padding='max_length',\n",
    "                        return_token_type_ids=True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt'\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "\n",
    "        # Combine the results into a dictionary\n",
    "        df_cp_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        return df_cp_inputs   \n",
    "    \n",
    "    elif model_type == 'cps': \n",
    "        # Extract the list of texts for tokenization of BERT_cp and BERT_cps model inputs\n",
    "        cps_texts = ds['Conclusion']+[\" \"]+ds['Premise']\n",
    "        stance = []\n",
    "\n",
    "        for text in cps_texts:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        padding='max_length',\n",
    "                        return_token_type_ids=True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt'\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        stance = torch.tensor(stance, dtype=torch.float)\n",
    "\n",
    "        # Combine the results into a dictionary\n",
    "        df_cps_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'stance': stance\n",
    "        }\n",
    "        return df_cps_inputs   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_c = tokenization(ds_training, stance_training, 'c')\n",
    "train_dataset_cp = tokenization(ds_training, stance_training, 'cp')\n",
    "train_dataset_cps = tokenization(ds_training, stance_training, 'cps')\n",
    "\n",
    "val_dataset_c = tokenization(ds_validation, stance_validation, 'c')\n",
    "val_dataset_cp = tokenization(ds_validation, stance_validation, 'cp')\n",
    "val_dataset_cps = tokenization(ds_validation, stance_validation, 'cps')\n",
    "\n",
    "test_dataset_c = tokenization(ds_test, stance_test, 'c')\n",
    "test_dataset_cp = tokenization(ds_test, stance_test, 'cp')\n",
    "test_dataset_cps = tokenization(ds_test, stance_test, 'cps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADER - BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def __len__(self):\\n        return len(self.labels)\\n    \\n    \\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DatasetCreator(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "'''\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset_c = DatasetCreator(train_dataset_c, labels_training)\n",
    "tr_dataset_cp = DatasetCreator(train_dataset_cp, labels_training)\n",
    "tr_dataset_cps = DatasetCreator(train_dataset_cps, labels_training)\n",
    "\n",
    "v_dataset_c = DatasetCreator(val_dataset_c, labels_validation)\n",
    "v_dataset_cp = DatasetCreator(val_dataset_cp, labels_validation)\n",
    "v_dataset_cps = DatasetCreator(val_dataset_cps, labels_validation)\n",
    "\n",
    "te_dataset_c = DatasetCreator(test_dataset_c, labels_test)\n",
    "te_dataset_cp = DatasetCreator(test_dataset_cp, labels_test)\n",
    "te_dataset_cps = DatasetCreator(test_dataset_cps, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  2057,  2323,  7221,  2529, 18856, 13369,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'labels': tensor([0, 0, 1, 0], dtype=torch.int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alili\\AppData\\Local\\Temp/ipykernel_17712/2651206841.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "print(tr_dataset_c.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaderc = DataLoader(tr_dataset_c, batch_size=batch_size)\n",
    "val_dataloaderc = DataLoader(v_dataset_c, batch_size=batch_size)\n",
    "test_dataloaderc = DataLoader(te_dataset_c, batch_size=batch_size)\n",
    "\n",
    "train_dataloadercp = DataLoader(tr_dataset_cp, batch_size=batch_size)\n",
    "val_dataloadercp = DataLoader(v_dataset_cp, batch_size=batch_size)\n",
    "test_dataloadercp = DataLoader(te_dataset_cp, batch_size=batch_size)\n",
    "\n",
    "train_dataloadercps = DataLoader(tr_dataset_cps, batch_size=batch_size)\n",
    "val_dataloadercps = DataLoader(v_dataset_cps, batch_size=batch_size)\n",
    "test_dataloadercps = DataLoader(te_dataset_cps, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: 9\n",
      "Attention Mask: 14\n",
      "Token Type IDs: 14\n",
      "Labels: 6\n",
      "Input IDs: 9\n",
      "Attention Mask: 14\n",
      "Token Type IDs: 14\n",
      "Labels: 6\n",
      "Input IDs: 9\n",
      "Attention Mask: 14\n",
      "Token Type IDs: 14\n",
      "Labels: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alili\\AppData\\Local\\Temp/ipykernel_17712/2651206841.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloaderc:\n",
    "    ids_batch, att_mask_batch, types_batch, label_batch = batch\n",
    "    print(\"Input IDs:\", len(ids_batch)) \n",
    "    print(\"Attention Mask:\", len(att_mask_batch))\n",
    "    print(\"Token Type IDs:\", len(types_batch))\n",
    "    print(\"Labels:\", len(label_batch))\n",
    "\n",
    "for batch in val_dataloaderc:\n",
    "    ids_batch, att_mask_batch, types_batch, label_batch = batch\n",
    "    print(\"Input IDs:\", len(ids_batch)) \n",
    "    print(\"Attention Mask:\", len(att_mask_batch))\n",
    "    print(\"Token Type IDs:\", len(types_batch))\n",
    "    print(\"Labels:\", len(label_batch))\n",
    "\n",
    "for batch in test_dataloaderc:\n",
    "    ids_batch, att_mask_batch, types_batch, label_batch = batch\n",
    "    print(\"Input IDs:\", len(ids_batch)) \n",
    "    print(\"Attention Mask:\", len(att_mask_batch))\n",
    "    print(\"Token Type IDs:\", len(types_batch))\n",
    "    print(\"Labels:\", len(label_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2 - MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random uniform classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_uniform_classifier(category):\n",
    "    \"\"\"\n",
    "    Creates a random classifier predicting 0 or 1 with uniform probability.\n",
    "    inputs:\n",
    "        category: Category to predict\n",
    "    outputs: \n",
    "        a function that generates random predictions\n",
    "    \"\"\"\n",
    "    def random_uniform_classifier(size):\n",
    "        \"\"\"\n",
    "        Generates random uniform predictions for the given category.\n",
    "        inputs: \n",
    "            size: number of predictions to generate\n",
    "        outputs: \n",
    "            array of random uniform predictions\n",
    "        \"\"\"\n",
    "        return np.random.choice([0, 1], size=size)\n",
    "    \n",
    "    return random_uniform_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_majority_classifier(category, majority_value):\n",
    "    \"\"\"\n",
    "    Creates a majority classifier always predicting the most frequent valorization for the column.\n",
    "    inputs:\n",
    "        category: Category to predict\n",
    "        majority_value: most frequent value (0 or 1)\n",
    "    outputs:\n",
    "        a function that generates majority predictions\n",
    "    \"\"\"\n",
    "    def majority_classifier(size):\n",
    "        \"\"\"\n",
    "        Generates majority predictions for the given category.\n",
    "        inputs: \n",
    "            size: number of predictions to generate\n",
    "        outputs: \n",
    "            array of majority predictions\n",
    "        \"\"\"\n",
    "        return np.full(size, majority_value)\n",
    "    \n",
    "    return majority_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the baseline models for every category and saving them in a classifiers dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "\n",
    "categories = ['Openess to change', 'Self-enhancement', 'Conservation', 'Self-transcendence']\n",
    "\n",
    "#create classifiers for each category and save them in the dictionary\n",
    "for category in categories:\n",
    "    #random uniform classifier\n",
    "    random_uniform_name = f'random_uniform_classifier_{category}'\n",
    "    classifiers[random_uniform_name] = create_random_uniform_classifier(category)\n",
    "\n",
    "    #majority classifier\n",
    "    majority_name = f'majority_classifier_{category}'\n",
    "    classifiers[majority_name] = create_majority_classifier(category, majority_value=1) #da capire perch√® majority_value=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT w/C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Carica la configurazione del modello\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "print(f\"Hidden size: {config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Verifica i dati di input: Assicurati che i dati di input (input_ids, attention_mask, token_type_ids) siano corretti e abbiano le dimensioni \n",
    "previste. Le dimensioni di input_ids dovrebbero essere [batch_size, outputs.length] quindi [16, 768]\n",
    "'''\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "print(f\"Token type IDs shape: {token_type_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass roBERTa(torch.nn.Module):\\n    def __init__(self, model_name):\\n        super(roBERTa, self).__init__()\\n        self.roberta = AutoModel.from_pretrained(model_name, return_dict=False)\\n        self.dropout = torch.nn.Dropout(p=0.3)\\n        self.classifier = torch.nn.Linear(output_channels, 4)\\n        \\n    def forward(self, ids, mask, token_type_ids):\\n        _, output = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)\\n        output = self.dropout(output)\\n        output = self.classifier(output)\\n        return output\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#definition of the C_Model class for the first BERT-based model\n",
    "class C_Model(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(C_Model, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            problem_type='multi_label_classification', \n",
    "            num_labels = 4, \n",
    "            return_dict=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.classifier = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        print(f\"Numero di valori restituiti: {len(outputs) if isinstance(outputs, tuple) else 1}\")\n",
    "        print(f\"Tipo outputs: {type(outputs)}\")\n",
    "        print(\"Output: \", outputs[0])\n",
    "        #print(f\"outputs logits: {outputs.logits}\")\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs\n",
    "\n",
    "'''\n",
    "class roBERTa(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(roBERTa, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_name, return_dict=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.classifier = torch.nn.Linear(output_channels, 4)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT w/CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the CP_Model class for the second BERT-based model\n",
    "class CP_Model(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CP_Model, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            problem_type='multi_label_classification', \n",
    "            num_labels = 4)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs.logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT w/CPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the CPS_Model class for the third BERT-based model\n",
    "\n",
    "class CPS_Model(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CPS_Model, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            problem_type='multi_label_classification', \n",
    "            num_labels = 4)\n",
    "        self.classifier = torch.nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, stance):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs.logits\n",
    "        stance = stance.unsqueeze(1).float() \n",
    "        combined_output = torch.cat((sequence_output, stance), dim=1)\n",
    "        logits = self.classifier(combined_output)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "c_model = C_Model(model_name)\n",
    "cp_model = CP_Model(model_name)\n",
    "cps_model = CPS_Model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_Model(\n",
      "  (bert): BertForSequenceClassification(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(c_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3 - METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining per-category F1 score metric\n",
    "def calculate_per_category_f1(y_true, y_pred):\n",
    "    categories = ['Openess to change', 'Self-enhancement', 'Conservation', 'Self-transcendence']\n",
    "    category_f1_scores = {}\n",
    "    for category in categories:\n",
    "        # Filter true and predicted labels for the current category\n",
    "        category_indices = [i for i, cat in enumerate(y_true['category']) if cat == category]\n",
    "        category_y_true = [y_true['Stance'][i] for i in category_indices]\n",
    "        category_y_pred = [y_pred[i] for i in category_indices]\n",
    "        \n",
    "        # Calculate F1 score for the current category\n",
    "        f1 = f1_score(category_y_true, category_y_pred, average='binary')\n",
    "        category_f1_scores[category] = f1\n",
    "    return category_f1_scores\n",
    "\n",
    "#defining macro F1 score metric\n",
    "def calculate_macro_f1(category_f1_scores):\n",
    "    average_f1 = np.mean(list(category_f1_scores.values()))\n",
    "    return average_f1\n",
    "\n",
    "#defining the EvalPrediction object for Trainer\n",
    "def calculate_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # F1 score per category\n",
    "    category_f1_scores = calculate_per_category_f1(labels, preds)\n",
    "    \n",
    "    # F1 score macro\n",
    "    macro_f1 = calculate_macro_f1(category_f1_scores)\n",
    "    \n",
    "    #results\n",
    "    result = {\n",
    "        'category_f1_scores': category_f1_scores,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4 - TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training phase utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the loss function\n",
    "def loss(outputs, targets):\n",
    "    return BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "#definition of the optimizers\n",
    "optimizer = Adam(c_model.parameters(), lr = 1e-5)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seeds = [42, 123, 2024]\n",
    "#seeds = [456]\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBert(model, dataloader, optimizer, loss):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"Size: {size}\")\n",
    "    model.train()\n",
    "    for batch, data in enumerate(dataloader, 0):\n",
    "        ids = data['input_ids'].to(dtype=torch.long)\n",
    "        print(f\"Input IDs: {ids.shape}\")\n",
    "        mask = data['attention_mask'].to(dtype=torch.long)\n",
    "        print(f\"Attention Mask: {mask.shape}\")\n",
    "        token_type_ids = data['token_type_ids'].to(dtype=torch.long)\n",
    "        labels = data['labels'].to(dtype=torch.float)\n",
    "        print(f\"Labels: {labels.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        if model == cps_model:\n",
    "            stance = data['stance'].to(dtype=torch.float)\n",
    "            print(f\"Stance: {stance.shape}\")\n",
    "            outputs = model(ids, mask, token_type_ids, stance)\n",
    "            print(f\"Outputs: {outputs.shape}\")\n",
    "        else:\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            print(f\"Outputs: {outputs.shape}\")\n",
    "            \n",
    "        loss = loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(ids)\n",
    "            print(f\"Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alili\\AppData\\Local\\Temp/ipykernel_17712/2651206841.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.0968,  0.3270, -1.3280,  1.1224],\n",
      "        [ 0.9729,  0.2507, -1.2706,  1.2562],\n",
      "        [ 0.8140,  0.8020, -0.8508,  1.5425]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.447778  [    0/    3]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 0.9967,  0.1621, -1.5715,  0.9273],\n",
      "        [ 1.0613,  0.2631, -1.5023,  0.7999],\n",
      "        [ 0.9110,  0.8472, -0.9868,  1.6037]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.435453  [    0/    3]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 0.8720,  0.1776, -1.4193,  1.0252],\n",
      "        [ 1.1129, -0.0188, -1.6687,  0.6340],\n",
      "        [ 0.6949,  1.0128, -0.9574,  1.5530]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.549011  [    0/    3]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.0312,  0.1629, -1.4292,  0.9707],\n",
      "        [ 1.0511,  0.2742, -2.0464,  0.8541],\n",
      "        [ 0.7053,  1.1187, -0.8064,  1.5824]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.518664  [    0/    3]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.1246,  0.3315, -1.5181,  0.8910],\n",
      "        [ 1.0734,  0.3024, -1.5826,  0.9979],\n",
      "        [ 0.8572,  0.8215, -1.0084,  1.2378]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.564667  [    0/    3]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.2362,  0.3219, -1.4943,  1.1052],\n",
      "        [ 1.2935,  0.4423, -1.4765,  1.0547],\n",
      "        [ 0.9312,  0.9296, -0.8497,  1.3239]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.397825  [    0/    3]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.0829,  0.2400, -1.4153,  0.9813],\n",
      "        [ 1.4979,  0.6061, -1.6865,  0.9637],\n",
      "        [ 0.8495,  1.0436, -0.8142,  1.3626]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.495650  [    0/    3]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.1903,  0.6514, -1.2822,  0.8499],\n",
      "        [ 1.2897,  0.4081, -1.6668,  1.0694],\n",
      "        [ 1.1222,  1.1070, -0.9624,  1.2648]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.497825  [    0/    3]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.2954,  0.2518, -1.4568,  1.0445],\n",
      "        [ 1.4016,  0.2742, -1.6696,  1.0199],\n",
      "        [ 0.9592,  1.0628, -0.8206,  1.6066]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.454914  [    0/    3]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Size: 3\n",
      "Input IDs: torch.Size([3, 100])\n",
      "Attention Mask: torch.Size([3, 100])\n",
      "Labels: torch.Size([3, 4])\n",
      "Numero di valori restituiti: 1\n",
      "Tipo outputs: <class 'tuple'>\n",
      "Output:  tensor([[ 1.2571,  0.4448, -1.1853,  1.1208],\n",
      "        [ 1.4784,  0.5128, -1.5735,  0.8943],\n",
      "        [ 0.8858,  1.1242, -0.7403,  1.4560]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: torch.Size([3, 4])\n",
      "Train loss: 0.391688  [    0/    3]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    trainBert(c_model, train_dataloaderc, optimizer, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
