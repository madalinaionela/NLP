{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9127527,"sourceType":"datasetVersion","datasetId":5510658}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#models\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import EvalPrediction\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.optim import Adam\n\n#metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:35:10.548799Z","iopub.execute_input":"2024-08-07T21:35:10.549501Z","iopub.status.idle":"2024-08-07T21:35:17.466410Z","shell.execute_reply.started":"2024-08-07T21:35:10.549468Z","shell.execute_reply":"2024-08-07T21:35:17.465260Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TASK 1 - CORPUS","metadata":{}},{"cell_type":"code","source":"def load_and_merge_data():\n    \n    #encodng the data into pandas.DataFrame objects\n    url_a_test = '/kaggle/input/dataset/arguments-test.tsv'\n    df_a_test = pd.read_csv(url_a_test, sep='\\t')\n\n    url_a_training = '/kaggle/input/dataset/arguments-training.tsv'\n    df_a_training = pd.read_csv(url_a_training, sep='\\t')\n\n    url_a_validation = '/kaggle/input/dataset/arguments-validation.tsv'\n    df_a_validation = pd.read_csv(url_a_validation, sep='\\t')\n\n    url_l_test = '/kaggle/input/dataset/labels-test.tsv'\n    df_l_test = pd.read_csv(url_l_test, sep='\\t')\n\n    url_l_training = '/kaggle/input/dataset/labels-training.tsv'\n    df_l_training = pd.read_csv(url_l_training, sep='\\t')\n\n    url_l_validation = '/kaggle/input/dataset/labels-validation.tsv'\n    df_l_validation = pd.read_csv(url_l_validation, sep='\\t')\n\n    #merge argument dataframes with label dataframes\n    df_test = pd.merge(df_a_test, df_l_test, on='Argument ID')\n    df_training = pd.merge(df_a_training, df_l_training, on='Argument ID')\n    df_validation = pd.merge(df_a_validation, df_l_validation, on='Argument ID')\n\n    return df_test, df_training, df_validation","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:03.836740Z","iopub.execute_input":"2024-08-07T21:37:03.837714Z","iopub.status.idle":"2024-08-07T21:37:03.846476Z","shell.execute_reply.started":"2024-08-07T21:37:03.837653Z","shell.execute_reply":"2024-08-07T21:37:03.845322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_test, df_training, df_validation = load_and_merge_data()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:10.402058Z","iopub.execute_input":"2024-08-07T21:37:10.402695Z","iopub.status.idle":"2024-08-07T21:37:10.535419Z","shell.execute_reply.started":"2024-08-07T21:37:10.402648Z","shell.execute_reply":"2024-08-07T21:37:10.534511Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def merge_and_drop_columns(df):\n    # Merge level 2 annotations to level 3 categories\n    df['Openess to change'] = df[['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism']].any(axis=1).astype(int)\n    df['Self-enhancement'] = df[['Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face']].any(axis=1).astype(int)\n    df['Conservation'] = df[['Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility']].any(axis=1).astype(int)\n    df['Self-transcendence'] = df[['Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity']].any(axis=1).astype(int)\n    \n    # Drop unuseful columns\n    columns_to_drop = ['Argument ID', 'Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity']\n    df = df.drop(columns=columns_to_drop)\n    \n    return df\n\ndf_test = merge_and_drop_columns(df_test)\ndf_training = merge_and_drop_columns(df_training)\ndf_validation = merge_and_drop_columns(df_validation)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:18.509880Z","iopub.execute_input":"2024-08-07T21:37:18.510529Z","iopub.status.idle":"2024-08-07T21:37:18.596639Z","shell.execute_reply.started":"2024-08-07T21:37:18.510487Z","shell.execute_reply":"2024-08-07T21:37:18.595501Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:23.373628Z","iopub.execute_input":"2024-08-07T21:37:23.374025Z","iopub.status.idle":"2024-08-07T21:37:23.391004Z","shell.execute_reply.started":"2024-08-07T21:37:23.373994Z","shell.execute_reply":"2024-08-07T21:37:23.390064Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                           Conclusion       Stance  \\\n0    We should end affirmative action      against   \n1    We should end affirmative action  in favor of   \n2           We should ban naturopathy  in favor of   \n3  We should prohibit women in combat  in favor of   \n4           We should ban naturopathy  in favor of   \n\n                                             Premise  Openess to change  \\\n0   affirmative action helps with employment equity.                  0   \n1  affirmative action can be considered discrimin...                  0   \n2  naturopathy is very dangerous for the most vul...                  0   \n3  women shouldn't be in combat because they aren...                  0   \n4  once eradicated illnesses are returning due to...                  0   \n\n   Self-enhancement  Conservation  Self-transcendence  \n0                 1             1                   1  \n1                 1             0                   1  \n2                 1             1                   1  \n3                 1             0                   0  \n4                 1             1                   1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>Openess to change</th>\n      <th>Self-enhancement</th>\n      <th>Conservation</th>\n      <th>Self-transcendence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We should end affirmative action</td>\n      <td>against</td>\n      <td>affirmative action helps with employment equity.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We should end affirmative action</td>\n      <td>in favor of</td>\n      <td>affirmative action can be considered discrimin...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We should ban naturopathy</td>\n      <td>in favor of</td>\n      <td>naturopathy is very dangerous for the most vul...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We should prohibit women in combat</td>\n      <td>in favor of</td>\n      <td>women shouldn't be in combat because they aren...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We should ban naturopathy</td>\n      <td>in favor of</td>\n      <td>once eradicated illnesses are returning due to...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### DATA EXPLORATION\nAncora da inserire","metadata":{}},{"cell_type":"markdown","source":"### DATA PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"Encoding 'Stance' column into numerical format  ","metadata":{}},{"cell_type":"code","source":"df_test['Stance'] = df_test['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)\ndf_training['Stance'] = df_training['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)\ndf_validation['Stance'] = df_validation['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:34.673637Z","iopub.execute_input":"2024-08-07T21:37:34.674656Z","iopub.status.idle":"2024-08-07T21:37:34.696976Z","shell.execute_reply.started":"2024-08-07T21:37:34.674623Z","shell.execute_reply":"2024-08-07T21:37:34.695878Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2140913224.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_test['Stance'] = df_test['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)\n/tmp/ipykernel_34/2140913224.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_training['Stance'] = df_training['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)\n/tmp/ipykernel_34/2140913224.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_validation['Stance'] = df_validation['Stance'].replace({'in favor of': 1, 'against': 0}).astype(str)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Preparing data for tokenization input","metadata":{}},{"cell_type":"code","source":"labels_test = df_test.iloc[:, 3:7].values\nlabels_training = df_training.iloc[:, 3:7].values\nlabels_validation = df_validation.iloc[:, 3:7].values\n\nstance_test = df_test['Stance'].values\nstance_training = df_training['Stance'].values\nstance_validation = df_validation['Stance'].values","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:37:57.076433Z","iopub.execute_input":"2024-08-07T21:37:57.076830Z","iopub.status.idle":"2024-08-07T21:37:57.085922Z","shell.execute_reply.started":"2024-08-07T21:37:57.076799Z","shell.execute_reply":"2024-08-07T21:37:57.084750Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Tokenization process and creation of a dataset structure compatible with the bert model ","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmax_length = 100\n\nclass BertDatasetCreator(Dataset):\n    def __init__(self, encodings, labels, tokenizer, max_length):\n        self.encodings = encodings\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.encodings)\n    \n    def __getitem__(self, idx):\n        item = str(self.encodings[idx])\n        item = ' '.join(item.split())\n        \n        encoded_dict = self.tokenizer.encode_plus(\n            item,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n        )\n        input_ids = encoded_dict['input_ids']\n        attention_masks = encoded_dict['attention_mask']\n        token_type_ids = encoded_dict['token_type_ids']\n\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_masks, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:12.088420Z","iopub.execute_input":"2024-08-07T21:40:12.088920Z","iopub.status.idle":"2024-08-07T21:40:13.093272Z","shell.execute_reply.started":"2024-08-07T21:40:12.088886Z","shell.execute_reply":"2024-08-07T21:40:13.092103Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbea78589fa14c43a252bd9c805d2663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d2d906ad0742f7988920405ba481f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2845e1f3f44d1885abd59b2aca474f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528d413778654f858277e7eb26af984e"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Applying the BertDatasetCreator and preparing the datasets for the three different type of BERT models","metadata":{}},{"cell_type":"markdown","source":"##### BERT w/C dataset","metadata":{}},{"cell_type":"code","source":"test_dataset_c = BertDatasetCreator(df_test['Conclusion'], labels_test, tokenizer, max_length)\ntrain_dataset_c = BertDatasetCreator(df_training['Conclusion'], labels_training, tokenizer, max_length)\nval_dataset_c = BertDatasetCreator(df_validation['Conclusion'], labels_validation, tokenizer, max_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:21.926513Z","iopub.execute_input":"2024-08-07T21:40:21.927294Z","iopub.status.idle":"2024-08-07T21:40:21.932948Z","shell.execute_reply.started":"2024-08-07T21:40:21.927262Z","shell.execute_reply":"2024-08-07T21:40:21.932000Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"DataLoader definition - which will supply the data to the neural network in batches for efficient training and processing","metadata":{}},{"cell_type":"code","source":"batch_size = 16\ntest_dataloader_c = DataLoader(test_dataset_c, batch_size=batch_size)\ntrain_dataloader_c = DataLoader(train_dataset_c, batch_size=batch_size)\nval_dataloader_c = DataLoader(val_dataset_c, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:25.305971Z","iopub.execute_input":"2024-08-07T21:40:25.306919Z","iopub.status.idle":"2024-08-07T21:40:25.313447Z","shell.execute_reply.started":"2024-08-07T21:40:25.306867Z","shell.execute_reply":"2024-08-07T21:40:25.312217Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"##### BERT w/CP","metadata":{}},{"cell_type":"code","source":"test_dataset_cp = BertDatasetCreator(df_test['Conclusion'] + ' ' + df_test['Premise'], labels_test, tokenizer, max_length)\ntrain_dataset_cp = BertDatasetCreator(df_training['Conclusion'] + ' ' + df_training['Premise'], labels_training, tokenizer, max_length)\nval_dataset_cp = BertDatasetCreator(df_validation['Conclusion'] + ' ' + df_validation['Premise'], labels_validation, tokenizer, max_length)\n\ntest_dataloader_cp = DataLoader(test_dataset_cp, batch_size=batch_size)\ntrain_dataloader_cp = DataLoader(train_dataset_cp, batch_size=batch_size)\nval_dataloader_cp = DataLoader(val_dataset_cp, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:29.584259Z","iopub.execute_input":"2024-08-07T21:40:29.584990Z","iopub.status.idle":"2024-08-07T21:40:29.596881Z","shell.execute_reply.started":"2024-08-07T21:40:29.584957Z","shell.execute_reply":"2024-08-07T21:40:29.595697Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"##### BERT w/CPS","metadata":{}},{"cell_type":"code","source":"test_dataset_cps = BertDatasetCreator(df_test['Conclusion'] + ' ' + df_test['Premise'] + ' ' + df_test['Stance'], labels_test, tokenizer, max_length)\ntrain_dataset_cps = BertDatasetCreator(df_training['Conclusion'] + ' ' + df_training['Premise'] + ' ' + df_training['Stance'], labels_training, tokenizer, max_length)\nval_dataset_cps = BertDatasetCreator(df_validation['Conclusion'] + ' ' + df_validation['Premise'] + ' ' + df_validation['Stance'], labels_validation, tokenizer, max_length)\n\ntest_dataloader_cps = DataLoader(test_dataset_cps, batch_size=batch_size)\ntrain_dataloader_cps = DataLoader(train_dataset_cps, batch_size=batch_size)\nval_dataloader_cps = DataLoader(val_dataset_cps, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:38.768007Z","iopub.execute_input":"2024-08-07T21:40:38.768399Z","iopub.status.idle":"2024-08-07T21:40:38.784511Z","shell.execute_reply.started":"2024-08-07T21:40:38.768369Z","shell.execute_reply":"2024-08-07T21:40:38.783527Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## TASK 2 - MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"### BASELINE MODELS","metadata":{}},{"cell_type":"markdown","source":"Random uniform classifier","metadata":{}},{"cell_type":"code","source":"def create_random_uniform_classifier(category):\n    \"\"\"\n    Creates a random classifier predicting 0 or 1 with uniform probability.\n    inputs:\n        category: Category to predict\n    outputs: \n        a function that generates random predictions\n    \"\"\"\n    def random_uniform_classifier(size):\n        \"\"\"\n        Generates random uniform predictions for the given category.\n        inputs: \n            size: number of predictions to generate\n        outputs: \n            array of random uniform predictions\n        \"\"\"\n        return np.random.choice([0, 1], size=size)\n    \n    return random_uniform_classifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority classifier","metadata":{}},{"cell_type":"code","source":"def create_majority_classifier(category, majority_value):\n    \"\"\"\n    Creates a majority classifier always predicting the most frequent valorization for the column.\n    inputs:\n        category: Category to predict\n        majority_value: most frequent value (0 or 1)\n    outputs:\n        a function that generates majority predictions\n    \"\"\"\n    def majority_classifier(size):\n        \"\"\"\n        Generates majority predictions for the given category.\n        inputs: \n            size: number of predictions to generate\n        outputs: \n            array of majority predictions\n        \"\"\"\n        return np.full(size, majority_value)\n    \n    return majority_classifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the baseline models for every category and saving them in a classifiers dictionary","metadata":{}},{"cell_type":"code","source":"classifiers = {}\n\ncategories = ['Openess to change', 'Self-enhancement', 'Conservation', 'Self-transcendence']\n\n#create classifiers for each category and save them in the dictionary\nfor category in categories:\n    #random uniform classifier\n    random_uniform_name = f'random_uniform_classifier_{category}'\n    classifiers[random_uniform_name] = create_random_uniform_classifier(category)\n\n    #majority classifier\n    majority_name = f'majority_classifier_{category}'\n    classifiers[majority_name] = create_majority_classifier(category, majority_value=1) #da capire perchè majority_value=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"class Bert_Model(torch.nn.Module):\n    def __init__(self):\n        super(Bert_Model, self).__init__()\n        self.bert = AutoModel.from_pretrained(\n            pretrained_model_name_or_path= 'bert-base-uncased', \n            problem_type='multi_label_classification', \n            num_labels = 4, \n            return_dict=False)\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 4)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        outputs = self.dropout(outputs)\n        outputs = self.classifier(outputs)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:49.534184Z","iopub.execute_input":"2024-08-07T21:40:49.535094Z","iopub.status.idle":"2024-08-07T21:40:49.543288Z","shell.execute_reply.started":"2024-08-07T21:40:49.535059Z","shell.execute_reply":"2024-08-07T21:40:49.542231Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Bert Models","metadata":{}},{"cell_type":"code","source":"c_model = Bert_Model()\ncp_model = Bert_Model()\ncps_model = Bert_Model()\n\nc_model.to(device)\ncp_model.to(device)\ncps_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:40:53.908617Z","iopub.execute_input":"2024-08-07T21:40:53.909229Z","iopub.status.idle":"2024-08-07T21:40:58.878642Z","shell.execute_reply.started":"2024-08-07T21:40:53.909201Z","shell.execute_reply":"2024-08-07T21:40:58.877640Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995e4a7e9d3f4990a0228d770c417253"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Bert_Model(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"print(c_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TASK 3 - METRICS","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef generate_classification_report(model, X_test, Y_test):\n    \n    # Mettere il modello in modalità di valutazione\n    model.eval()\n    \n    # Disabilitare il calcolo dei gradienti per la valutazione\n    with torch.no_grad():\n        # Predire le etichette utilizzando il modello\n        Y_pred = model(X_test['input_ids'], X_test['attention_mask'], X_test['token_type_ids'])\n        \n        # Convertire le predizioni in numpy array\n        Y_pred = Y_pred.detach().cpu().numpy()\n    \n    # Generare il classification report\n    report = classification_report(Y_test, Y_pred, output_dict=True)\n    \n    # Stampare il report\n    print(\"Classification Report:\")\n    print(report)\n    \n    # Estrarre e stampare il macro F1 score\n    macro_f1_score = report['macro avg']['f1-score']\n    print(f\"Macro F1 Score: {macro_f1_score}\")\n\n# Esempio di utilizzo:\n# Supponiamo che c_model, X_test e Y_test siano già definiti\n# generate_classification_report(c_model, X_test, Y_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:00.573557Z","iopub.execute_input":"2024-08-07T22:34:00.574300Z","iopub.status.idle":"2024-08-07T22:34:00.581725Z","shell.execute_reply.started":"2024-08-07T22:34:00.574265Z","shell.execute_reply":"2024-08-07T22:34:00.580751Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## TASK 4 - TRAINING AND EVALUATION","metadata":{}},{"cell_type":"markdown","source":"Training process utils","metadata":{}},{"cell_type":"code","source":"#definition of the loss function\ndef loss_function(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\n#definition of the optimizers\noptimizer = Adam(c_model.parameters(), lr = 1e-5)\n\n# Set seeds for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n\n#seeds = [42, 123, 2024]\nseeds = 456\nset_seed(seeds)\n\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:57:33.464333Z","iopub.execute_input":"2024-08-07T21:57:33.464748Z","iopub.status.idle":"2024-08-07T21:57:33.474310Z","shell.execute_reply.started":"2024-08-07T21:57:33.464715Z","shell.execute_reply":"2024-08-07T21:57:33.473230Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Training function definition","metadata":{}},{"cell_type":"code","source":"def trainBert(model, dataloader, optimizer, loss_function):\n    size = len(dataloader.dataset)\n    model.train()\n    running_loss = 0.0\n    for batch, data in enumerate(dataloader, 0):\n        ids = data['input_ids'].to(device, dtype=torch.long)\n        mask = data['attention_mask'].to(device, dtype=torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n        labels = data['labels'].to(device)\n        optimizer.zero_grad()\n        outputs = model(ids, mask, token_type_ids)\n    \n        loss_value = loss_function(outputs, labels)\n        loss_value.backward()\n        optimizer.step()\n        running_loss += loss_value.item()\n        avg_train_loss = running_loss / len(dataloader)\n  \n        if batch % 100 == 0:\n            loss_value, current = loss_value.item(), batch * len(ids)\n            print(f\"Train loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")\n    \n    return avg_train_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:42:35.450797Z","iopub.execute_input":"2024-08-07T21:42:35.451510Z","iopub.status.idle":"2024-08-07T21:42:35.461481Z","shell.execute_reply.started":"2024-08-07T21:42:35.451479Z","shell.execute_reply":"2024-08-07T21:42:35.460234Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Validation function definition","metadata":{}},{"cell_type":"code","source":"\ndef validate_model(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for batch_idx, data in enumerate(dataloader, 0):\n            input_ids = data['input_ids'].to(device, dtype = torch.long)\n            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            labels = data['labels'].to(device, dtype = torch.float)\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            all_labels.extend(labels.cpu().numpy())\n            all_outputs.extend(outputs.cpu().numpy())\n\n    all_labels = np.array(all_labels)\n    all_outputs = np.array(all_outputs)\n\n    # Numero di etichette\n    num_labels = all_labels.shape[1]\n\n    # Inizializza una lista per memorizzare le soglie ottimali per ogni etichetta\n    optimal_thresholds = []\n    auc_scores = []\n\n    for i in range(num_labels):\n        # Calcola la curva ROC\n        fpr, tpr, thresholds = roc_curve(all_labels[:, i], all_outputs[:, i])\n        # Calcola l'AUC\n        roc_auc = auc(fpr, tpr)\n        auc_scores.append(roc_auc)\n        # Trova la soglia che massimizza la somma di sensibilità e specificità\n        optimal_idx = np.argmax(tpr - fpr)\n        optimal_threshold = thresholds[optimal_idx]\n        optimal_thresholds.append(optimal_threshold)\n\n    # Applica le soglie ottimali per ottenere le previsioni binarie\n    all_preds = np.zeros_like(all_outputs)\n    for i in range(num_labels):\n        all_preds[:, i] = (all_outputs[:, i] > optimal_thresholds[i]).astype(int)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n\n     # Stampa le AUC per ogni etichetta\n    for i, auc_score in enumerate(auc_scores):\n        print(f'AUC for label {i}: {auc_score}')\n\n    return accuracy, precision, recall, f1, optimal_thresholds\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:52:52.148010Z","iopub.execute_input":"2024-08-07T21:52:52.148780Z","iopub.status.idle":"2024-08-07T21:52:52.162124Z","shell.execute_reply.started":"2024-08-07T21:52:52.148748Z","shell.execute_reply":"2024-08-07T21:52:52.161044Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### TRAINING BERT W/C","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n    avg_train_loss_c = trainBert(c_model, train_dataloader_c, optimizer, loss_function)\n    print(f'Loss: {avg_train_loss:.4f}')\n    #validation\n    accuracy_c, precision_c, recall_c, f1_c, c_thresholds = validate_model(c_model, val_dataloader_c)\n    print(f'Validation - Accuracy: {accuracy_c:.4f}, Precision: {precision_c:.4f}, Recall: {recall_:.4f}, F1 Score: {f1_c:.4f}')\n\ntorch.save(c_model.state_dict(), 'model.pthc')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T21:58:43.224060Z","iopub.execute_input":"2024-08-07T21:58:43.224780Z","iopub.status.idle":"2024-08-07T22:16:15.352562Z","shell.execute_reply.started":"2024-08-07T21:58:43.224745Z","shell.execute_reply":"2024-08-07T22:16:15.351203Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nTrain loss: 0.593304  [    0/ 5393]\nTrain loss: 0.559206  [ 1600/ 5393]\nTrain loss: 0.536918  [ 3200/ 5393]\nTrain loss: 0.579860  [ 4800/ 5393]\nLoss: 0.5478\nAUC for label 0: 0.5219432100300885\nAUC for label 1: 0.6431367946934008\nAUC for label 2: 0.4952620035212319\nAUC for label 3: 0.5146610140634045\nValidation - Accuracy: 0.1403, Precision: 0.7008, Recall: 0.6153, F1 Score: 0.6416\nEpoch 2\n-------------------------------\nTrain loss: 0.624622  [    0/ 5393]\nTrain loss: 0.549056  [ 1600/ 5393]\nTrain loss: 0.539200  [ 3200/ 5393]\nTrain loss: 0.526498  [ 4800/ 5393]\nLoss: 0.5479\nAUC for label 0: 0.5386101955982032\nAUC for label 1: 0.6604704186155679\nAUC for label 2: 0.5516912357136463\nAUC for label 3: 0.5233748765621276\nValidation - Accuracy: 0.1060, Precision: 0.7342, Recall: 0.4895, F1 Score: 0.5528\nEpoch 3\n-------------------------------\nTrain loss: 0.596656  [    0/ 5393]\nTrain loss: 0.534782  [ 1600/ 5393]\nTrain loss: 0.552169  [ 3200/ 5393]\nTrain loss: 0.468574  [ 4800/ 5393]\nLoss: 0.5430\nAUC for label 0: 0.5429643962478056\nAUC for label 1: 0.6415508502517505\nAUC for label 2: 0.5396399689654143\nAUC for label 3: 0.4870534954200293\nValidation - Accuracy: 0.1108, Precision: 0.7214, Recall: 0.5440, F1 Score: 0.5742\nEpoch 4\n-------------------------------\nTrain loss: 0.565239  [    0/ 5393]\nTrain loss: 0.529506  [ 1600/ 5393]\nTrain loss: 0.547544  [ 3200/ 5393]\nTrain loss: 0.448523  [ 4800/ 5393]\nLoss: 0.5391\nAUC for label 0: 0.5466130274430642\nAUC for label 1: 0.6340346583066494\nAUC for label 2: 0.5582010384649817\nAUC for label 3: 0.5085282800422243\nValidation - Accuracy: 0.0844, Precision: 0.7257, Recall: 0.5198, F1 Score: 0.5427\nEpoch 5\n-------------------------------\nTrain loss: 0.582978  [    0/ 5393]\nTrain loss: 0.546952  [ 1600/ 5393]\nTrain loss: 0.530962  [ 3200/ 5393]\nTrain loss: 0.409953  [ 4800/ 5393]\nLoss: 0.5358\nAUC for label 0: 0.5469873380179956\nAUC for label 1: 0.6513191056569821\nAUC for label 2: 0.5026073528095253\nAUC for label 3: 0.5164027650083427\nValidation - Accuracy: 0.1936, Precision: 0.7045, Recall: 0.7550, F1 Score: 0.7245\nEpoch 6\n-------------------------------\nTrain loss: 0.599846  [    0/ 5393]\nTrain loss: 0.534862  [ 1600/ 5393]\nTrain loss: 0.528798  [ 3200/ 5393]\nTrain loss: 0.411657  [ 4800/ 5393]\nLoss: 0.5344\nAUC for label 0: 0.5244414042506375\nAUC for label 1: 0.6579657664001073\nAUC for label 2: 0.5584890036107548\nAUC for label 3: 0.5318827255082235\nValidation - Accuracy: 0.1566, Precision: 0.7074, Recall: 0.5296, F1 Score: 0.5911\nEpoch 7\n-------------------------------\nTrain loss: 0.593094  [    0/ 5393]\nTrain loss: 0.552165  [ 1600/ 5393]\nTrain loss: 0.545398  [ 3200/ 5393]\nTrain loss: 0.381249  [ 4800/ 5393]\nLoss: 0.5326\nAUC for label 0: 0.5325040301170528\nAUC for label 1: 0.6639954846965861\nAUC for label 2: 0.5337284772164363\nAUC for label 3: 0.516418088330439\nValidation - Accuracy: 0.1108, Precision: 0.7177, Recall: 0.5274, F1 Score: 0.5816\nEpoch 8\n-------------------------------\nTrain loss: 0.595970  [    0/ 5393]\nTrain loss: 0.514941  [ 1600/ 5393]\nTrain loss: 0.530153  [ 3200/ 5393]\nTrain loss: 0.338554  [ 4800/ 5393]\nLoss: 0.5316\nAUC for label 0: 0.5291842660403442\nAUC for label 1: 0.6300178265072899\nAUC for label 2: 0.5829764853331743\nAUC for label 3: 0.5309820546872339\nValidation - Accuracy: 0.0812, Precision: 0.7304, Recall: 0.4190, F1 Score: 0.5187\nEpoch 9\n-------------------------------\nTrain loss: 0.584723  [    0/ 5393]\nTrain loss: 0.535030  [ 1600/ 5393]\nTrain loss: 0.557051  [ 3200/ 5393]\nTrain loss: 0.324282  [ 4800/ 5393]\nLoss: 0.5297\nAUC for label 0: 0.5493133254564676\nAUC for label 1: 0.647893510368992\nAUC for label 2: 0.5394057175255886\nAUC for label 3: 0.517020805666224\nValidation - Accuracy: 0.1888, Precision: 0.7012, Recall: 0.7519, F1 Score: 0.7252\nEpoch 10\n-------------------------------\nTrain loss: 0.595351  [    0/ 5393]\nTrain loss: 0.531700  [ 1600/ 5393]\nTrain loss: 0.530364  [ 3200/ 5393]\nTrain loss: 0.318229  [ 4800/ 5393]\nLoss: 0.5281\nAUC for label 0: 0.522229025453119\nAUC for label 1: 0.6046259507004867\nAUC for label 2: 0.508503924084629\nAUC for label 3: 0.5197534647733851\nValidation - Accuracy: 0.2284, Precision: 0.7122, Recall: 0.6804, F1 Score: 0.6806\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Printing the classification_report for BERT w/C","metadata":{}},{"cell_type":"code","source":"generate_classification_report(c_model, test_dataset_c, labels_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:12.562616Z","iopub.execute_input":"2024-08-07T22:34:12.563038Z","iopub.status.idle":"2024-08-07T22:34:12.692663Z","shell.execute_reply.started":"2024-08-07T22:34:12.563005Z","shell.execute_reply":"2024-08-07T22:34:12.691079Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_classification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_test\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[41], line 11\u001b[0m, in \u001b[0;36mgenerate_classification_report\u001b[0;34m(model, X_test, Y_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Disabilitare il calcolo dei gradienti per la valutazione\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Predire le etichette utilizzando il modello\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m model(\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Convertire le predizioni in numpy array\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m Y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n","Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mBertDatasetCreator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 15\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     16\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(item\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     18\u001b[0m     encoded_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m     19\u001b[0m         item,\n\u001b[1;32m     20\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n","\u001b[0;31mKeyError\u001b[0m: 'input_ids'"],"ename":"KeyError","evalue":"'input_ids'","output_type":"error"}]},{"cell_type":"markdown","source":"### TRAINING BERT W/CP","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n    avg_train_loss_cp = trainBert(cp_model, train_dataloader_cp, optimizer, loss_function)\n    print(f'Loss: {avg_train_loss_cp:.4f}')\n    #validation\n    accuracy_cp, precision_cp, recall_cp, f1_cp, cp_thresholds = validate_model(cp_model, val_dataloader_cp)\n    print(f'Validation - Accuracy: {accuracy_cp:.4f}, Precision: {precision_cp:.4f}, Recall: {recall_cp:.4f}, F1 Score: {f1_cp:.4f}')\n\ntorch.save(cp_model.state_dict(), 'model.pthcp')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:40:12.889591Z","iopub.execute_input":"2024-08-07T22:40:12.889978Z","iopub.status.idle":"2024-08-07T22:56:31.694351Z","shell.execute_reply.started":"2024-08-07T22:40:12.889947Z","shell.execute_reply":"2024-08-07T22:56:31.693270Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nTrain loss: 0.627758  [    0/ 5393]\nTrain loss: 0.633907  [ 1600/ 5393]\nTrain loss: 0.658979  [ 3200/ 5393]\nTrain loss: 0.625966  [ 4800/ 5393]\nLoss: 0.6745\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 2\n-------------------------------\nTrain loss: 0.638482  [    0/ 5393]\nTrain loss: 0.633286  [ 1600/ 5393]\nTrain loss: 0.644881  [ 3200/ 5393]\nTrain loss: 0.622176  [ 4800/ 5393]\nLoss: 0.6718\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 3\n-------------------------------\nTrain loss: 0.638155  [    0/ 5393]\nTrain loss: 0.618516  [ 1600/ 5393]\nTrain loss: 0.627788  [ 3200/ 5393]\nTrain loss: 0.660560  [ 4800/ 5393]\nLoss: 0.6731\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 4\n-------------------------------\nTrain loss: 0.649691  [    0/ 5393]\nTrain loss: 0.648878  [ 1600/ 5393]\nTrain loss: 0.648774  [ 3200/ 5393]\nTrain loss: 0.660970  [ 4800/ 5393]\nLoss: 0.6717\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 5\n-------------------------------\nTrain loss: 0.632333  [    0/ 5393]\nTrain loss: 0.647737  [ 1600/ 5393]\nTrain loss: 0.620834  [ 3200/ 5393]\nTrain loss: 0.630736  [ 4800/ 5393]\nLoss: 0.6743\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 6\n-------------------------------\nTrain loss: 0.639836  [    0/ 5393]\nTrain loss: 0.628336  [ 1600/ 5393]\nTrain loss: 0.616839  [ 3200/ 5393]\nTrain loss: 0.626282  [ 4800/ 5393]\nLoss: 0.6739\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 7\n-------------------------------\nTrain loss: 0.632169  [    0/ 5393]\nTrain loss: 0.624272  [ 1600/ 5393]\nTrain loss: 0.648153  [ 3200/ 5393]\nTrain loss: 0.646484  [ 4800/ 5393]\nLoss: 0.6726\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 8\n-------------------------------\nTrain loss: 0.630556  [    0/ 5393]\nTrain loss: 0.626105  [ 1600/ 5393]\nTrain loss: 0.643574  [ 3200/ 5393]\nTrain loss: 0.632720  [ 4800/ 5393]\nLoss: 0.6731\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 9\n-------------------------------\nTrain loss: 0.639804  [    0/ 5393]\nTrain loss: 0.638229  [ 1600/ 5393]\nTrain loss: 0.629292  [ 3200/ 5393]\nTrain loss: 0.635926  [ 4800/ 5393]\nLoss: 0.6728\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\nEpoch 10\n-------------------------------\nTrain loss: 0.634686  [    0/ 5393]\nTrain loss: 0.640048  [ 1600/ 5393]\nTrain loss: 0.641636  [ 3200/ 5393]\nTrain loss: 0.630718  [ 4800/ 5393]\nLoss: 0.6717\nAUC for label 0: 0.49786595137071815\nAUC for label 1: 0.5219590157979737\nAUC for label 2: 0.46428262361612604\nAUC for label 3: 0.4591301460823373\nValidation - Accuracy: 0.0401, Precision: 0.7302, Recall: 0.5043, F1 Score: 0.4236\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### TRAINING BERT W/CPS","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n    avg_train_loss_cps = trainBert(cps_model, train_dataloader_cps, optimizer, loss_function)\n    print(f'Loss: {avg_train_loss_cps:.4f}')\n    #validation\n    accuracy_cps, precision_cps, recall_cps, f1_cps, cps_thresholds = validate_model(cps_model, val_dataloader_cps)\n    print(f'Validation - Accuracy: {accuracy_cps:.4f}, Precision: {precision_cps:.4f}, Recall: {recall_cps:.4f}, F1 Score: {f1_cps:.4f}')\n\ntorch.save(cps_model.state_dict(), 'model.pthcps')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:57:05.383905Z","iopub.execute_input":"2024-08-07T22:57:05.384600Z","iopub.status.idle":"2024-08-07T23:13:24.362785Z","shell.execute_reply.started":"2024-08-07T22:57:05.384567Z","shell.execute_reply":"2024-08-07T23:13:24.361812Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nTrain loss: 0.699721  [    0/ 5393]\nTrain loss: 0.688629  [ 1600/ 5393]\nTrain loss: 0.676356  [ 3200/ 5393]\nTrain loss: 0.710600  [ 4800/ 5393]\nLoss: 0.7335\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 2\n-------------------------------\nTrain loss: 0.751042  [    0/ 5393]\nTrain loss: 0.678805  [ 1600/ 5393]\nTrain loss: 0.676573  [ 3200/ 5393]\nTrain loss: 0.703097  [ 4800/ 5393]\nLoss: 0.7355\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 3\n-------------------------------\nTrain loss: 0.681916  [    0/ 5393]\nTrain loss: 0.698266  [ 1600/ 5393]\nTrain loss: 0.688345  [ 3200/ 5393]\nTrain loss: 0.703556  [ 4800/ 5393]\nLoss: 0.7355\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 4\n-------------------------------\nTrain loss: 0.684475  [    0/ 5393]\nTrain loss: 0.677533  [ 1600/ 5393]\nTrain loss: 0.658475  [ 3200/ 5393]\nTrain loss: 0.676829  [ 4800/ 5393]\nLoss: 0.7363\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 5\n-------------------------------\nTrain loss: 0.712749  [    0/ 5393]\nTrain loss: 0.698521  [ 1600/ 5393]\nTrain loss: 0.697318  [ 3200/ 5393]\nTrain loss: 0.689366  [ 4800/ 5393]\nLoss: 0.7349\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 6\n-------------------------------\nTrain loss: 0.706086  [    0/ 5393]\nTrain loss: 0.679954  [ 1600/ 5393]\nTrain loss: 0.701878  [ 3200/ 5393]\nTrain loss: 0.674279  [ 4800/ 5393]\nLoss: 0.7363\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 7\n-------------------------------\nTrain loss: 0.711476  [    0/ 5393]\nTrain loss: 0.701946  [ 1600/ 5393]\nTrain loss: 0.679793  [ 3200/ 5393]\nTrain loss: 0.691901  [ 4800/ 5393]\nLoss: 0.7372\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 8\n-------------------------------\nTrain loss: 0.699062  [    0/ 5393]\nTrain loss: 0.700865  [ 1600/ 5393]\nTrain loss: 0.660404  [ 3200/ 5393]\nTrain loss: 0.714599  [ 4800/ 5393]\nLoss: 0.7361\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 9\n-------------------------------\nTrain loss: 0.744335  [    0/ 5393]\nTrain loss: 0.684532  [ 1600/ 5393]\nTrain loss: 0.714485  [ 3200/ 5393]\nTrain loss: 0.716218  [ 4800/ 5393]\nLoss: 0.7360\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\nEpoch 10\n-------------------------------\nTrain loss: 0.689353  [    0/ 5393]\nTrain loss: 0.683242  [ 1600/ 5393]\nTrain loss: 0.696683  [ 3200/ 5393]\nTrain loss: 0.665271  [ 4800/ 5393]\nLoss: 0.7355\nAUC for label 0: 0.5163626339984023\nAUC for label 1: 0.46930878975339063\nAUC for label 2: 0.45422771627226877\nAUC for label 3: 0.498875438417271\nValidation - Accuracy: 0.0327, Precision: 0.7634, Recall: 0.2412, F1 Score: 0.2844\n","output_type":"stream"}]}]}