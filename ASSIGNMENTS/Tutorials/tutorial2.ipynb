{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rduSZj7b5eiP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sparse/Dense Word embeddings, Embedding visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3wzWLL-LiKd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART 0 ($\\sim$5 mins)\n",
    "*   Downloading a **dataset**.\n",
    "*   Encoding a a **dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl48Am5trp3Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART I ($\\sim$30 mins)\n",
    "*   Building a **vocabulary**.\n",
    "*   Building a **word-word co-occurrence matrix**.\n",
    "*   Defining a **similarity metric**: cosine similarity.\n",
    "*   Embedding **visualization** and **analysis** of their semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4anSmM4rp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART II ($\\sim$30 mins)\n",
    "\n",
    "*   Loading pre-trained **dense** word embeddings: Word2Vec, GloVe, FastText.\n",
    "*   Checking **out-of-vocabulary** (OOV) terms.\n",
    "*   **Handling** OOV terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4-E45fvrp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First of all, we need to import some useful packages that we will use during this hands-on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUXZLYya69wc",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import sys\n",
    "\n",
    "# data and numerical management packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# useful during debugging (progress bars)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# typing\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 2560,\n",
    "        'height': 1440,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qImKj2Mu7LCX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We will use the IMDB dataset first introduced in tutorial 1.\n",
    "\n",
    "* [**Stats**] A dataset of 50k sentences used for sentiment analysis: 25k with positive sentiment, 25k with negative one.\n",
    "* [**Labels**] We ignore sentiment labels since we focus on learning a proper word embedding representation.\n",
    "\n",
    "We start by **downloading** the dataset and **extract** it to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSvqBcKJ7iTY",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with tarfile.open(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_name = \"aclImdb\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"Movies.tar.gz\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3FVyXxr7llG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Inspection\n",
    "\n",
    "Feel free to check the dataset folder content!\n",
    "\n",
    "Usually, the README file is a good starting point (if it exists and it is informative -- which is not so common!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv3NW1SNrp3a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Format\n",
    "\n",
    "Just like in the first assignment, we need a **high level view** of the dataset that is helpful to our needs. \n",
    "\n",
    "We encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P05YfYCe7qCj",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder = dataset_folder.joinpath(dataset_name, split, sentiment)\n",
    "        for file_path in folder.glob('*.txt'):            \n",
    "            with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "                text = text_file.read()\n",
    "                score = file_path.stem.split(\"_\")[1]\n",
    "                score = int(score)\n",
    "                file_id = file_path.stem.split(\"_\")[0]\n",
    "\n",
    "                num_sentiment = 1 if sentiment == 'pos' else 0\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"score\": score,\n",
    "                    \"sentiment\": num_sentiment,\n",
    "                    \"split\": split,\n",
    "                    \"text\": text\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\", dataset_name)\n",
    "if not folder.exists():\n",
    "    folder.mkdir(parents=True)\n",
    "\n",
    "# transform the list of rows in a proper dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "df = df[[\"file_id\", \n",
    "         \"score\",\n",
    "         \"sentiment\",\n",
    "         \"split\",\n",
    "         \"text\"]\n",
    "       ]\n",
    "df_path = folder.with_name(dataset_name + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va6D4sBv74mF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect some major details of the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3a3hqAP7-6r",
    "outputId": "65102475-2651-464d-89f2-e56051f92866",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset size: {df.shape}\") # (50000, 5)\n",
    "print(f\"Dataset columns: {df.columns.values}\") # ['file_id', 'score', 'sentiment', 'split', 'text]\n",
    "print(f\"Classes distribution:\\n{df.sentiment.value_counts()}\") # [0: 25000, 1: 25000]\n",
    "print(f\"Some examples: {df.iloc[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjf3k-qVrp3b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART I\n",
    "*   Building a **vocabulary**.\n",
    "*   Building a **word-word co-occurrence matrix**.\n",
    "*   Defining a **similarity metric**: cosine similarity.\n",
    "*   Embedding **visualization** and **analysis** of their semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sz1WJqE6nVg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Building a vocabulary\n",
    "\n",
    "We consider words as the atomic units for text representation: each word will be associated with a numeric representation.\n",
    "\n",
    "At this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\n",
    "\n",
    "**Definition**: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned to an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**: Suppose you have the given toy corpus $D$: { \"the cat is on the table\" }\n",
    "\n",
    "As you notice, the dataset is comprised of only one sentence: \"the cat is on the table\". The corresponding vocabulary (a possible one) will be:\n",
    "\n",
    "V = {0: 'the', 1: 'cat', 2: 'is', 3: 'on', 4: 'table'}\n",
    "\n",
    "In this case, indexing follows word order, but it is not mandatory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5qeqzylrp3b",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Note 1\n",
    "\n",
    "The most important thing to remember is that the vocabulary **should always be the same one**.\n",
    "\n",
    "$\\rightarrow$ Thus, make sure that the vocabulary creation routine always returns the same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Note 2\n",
    "\n",
    "A vocabulary is **exclusively defined** by the tokenization step you define!\n",
    "\n",
    "$\\rightarrow$ Characters, sub-words, words are examples of granularity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J59S-tMg8a1X",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Text pre-processing\n",
    "\n",
    "Before vocabulary creation, we have to do a little bit of **text pre-processing** so as to avoid spurious data.\n",
    "\n",
    "$\\rightarrow$ Data quality is one of the crucial factors that lead to better performance.\n",
    "\n",
    "$\\rightarrow$ Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjD3T_Nnrp3c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Types of pre-processing**: there are a lot of pre-processing steps that we can consider, either general or quite task- specific.\n",
    "\n",
    "*    **Text to lower**: casing usually doesn't affect our task, but in some scenarios, such as part-of-speech tagging, it is crucial.\n",
    "\n",
    "*    **Replace special characters**: special characters are usually employed as variants of a single character like the spacing symbol ' '. In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\n",
    "\n",
    "*    **Text stripping**: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as 'apple' and ' apple '."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on.\n",
    "\n",
    "$\\rightarrow$ If you are interested you can check [here](https://medium.com/swlh/text-normalization-7ecc8e084e31) and [here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) some good blogs about the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uCREu9urp3c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7HqhEjl8cqg",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces special characters, such as paranthesis, with spacing character\n",
    "    \"\"\"\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def replace_br(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces br characters\n",
    "    \"\"\"\n",
    "    return text.replace('br', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bePDQapQrp3c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          replace_special_characters,\n",
    "                          replace_br,\n",
    "                          filter_out_uncommon_symbols,\n",
    "                          remove_stopwords,\n",
    "                          strip_text\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36RFsheCrp3c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Pre-processing text...')\n",
    "\n",
    "print()\n",
    "print(f'[Debug] Before:\\n{df.text.values[50]}')\n",
    "print()\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(f'[Debug] After:\\n{df.text.values[50]}')\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk_mzom18yLV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Vocabulary Creation\n",
    "\n",
    "Since the text has been pre-processed, space splitting should work correctly. <br>\n",
    "We proceed on building the vocabulary and perform some sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37jKjW3arp3c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check [keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)). \n",
    "\n",
    "**Note**: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjhjcZKg82-U",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],\n",
    "                                           Dict[str, int],\n",
    "                                           List[str]):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 0\n",
    "    for sentence in tqdm(df.text.values):\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = curr_idx\n",
    "                idx_to_word[curr_idx] = token\n",
    "                curr_idx += 1\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n",
    "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word)}')\n",
    "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx)}')\n",
    "print(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10) + 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqY9afATrp3d",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n",
    "                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n",
    "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
    "    assert len(idx_to_word) == len(word_to_idx)\n",
    "    assert len(idx_to_word) == len(word_listing)\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
    "    for i in tqdm(range(0, len(idx_to_word))):\n",
    "        assert idx_to_word[i] in word_to_idx\n",
    "        assert word_to_idx[idx_to_word[i]] == i\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
    "    _, _, first_word_listing = build_vocabulary(df)\n",
    "    _, _, second_word_listing = build_vocabulary(df)\n",
    "    assert first_word_listing == second_word_listing\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
    "    toy_df = pd.DataFrame.from_dict({\n",
    "        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
    "    })\n",
    "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
    "    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n",
    "    assert set(toy_word_listing) == toy_valid_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary evaluation...\")\n",
    "evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\n",
    "print(\"Evaluation completed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoSJR73EF2Tf",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Note\n",
    "Define **intermediary tests** for your code in order to inspect data and to assess the correctness of your code!\n",
    "\n",
    "$\\rightarrow$ You don't want to **re-run** huge and time-consuming experiments due to early pipeline errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnQ4bvP86-w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Saving the vocabulary\n",
    "\n",
    "Generally speaking, it is a good idea to save the dictionary in clear format.\n",
    "\n",
    "$\\rightarrow$ In this way you can quickly check for errors or useful words.\n",
    "\n",
    "In this case, we will save the vocabulary dictionary in **JSON format**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i51oflz4WVI",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlPOkSbv8_Fv",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import simplejson as sj\n",
    "\n",
    "vocab_path = Path.cwd().joinpath('Datasets', \"aclImdb\", 'vocab.json')\n",
    "\n",
    "print(f\"Saving vocabulary to {vocab_path}\")\n",
    "with vocab_path.open(mode='w') as f:\n",
    "    sj.dump(word_to_idx, f, indent=4)\n",
    "print(\"Saving completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Spend some time at checking the built vocabulary!\n",
    "\n",
    "In many tasks (e.g., sequential tagging, embedding analysis, etc...), the way text is tokenized is crucial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2POKdKl7A28",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Sparse embeddings\n",
    "\n",
    "Working with text inherently requires a numerical conversion step, known as **embedding**.\n",
    "\n",
    "**Bag-of-Words (BoW)**\n",
    "1.   Count the occurrence of each word in a given corpus\n",
    "2.   Build a word co-occurrence matrix: useful to identify the most common terms in each given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of reasoning is directly related to **how meaning is assigned to words**.\n",
    "\n",
    "$\\rightarrow$  In particular, it is the environment enclosing a word that gives a specific meaning to it.\n",
    "\n",
    "$\\rightarrow$  Thus, we look for numerical encoding methods that reflect such point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5lllkXW8MN-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 A quick simplification\n",
    "\n",
    "Since the dataset is quite large, the co-occurrence matrix construction may take a while or may require efficient solutions.\n",
    "\n",
    "For the purpose of this tutorial, we can rely on a **small slice** of the dataset.\n",
    "\n",
    "$\\rightarrow$ Nonetheless, feel free to work with the whole dataset! Suggestions on how to handle this scenario are\n",
    "given below when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muKUp_f_8VYz",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This type of slicing is not mandatory, but it is sufficient to our purposes\n",
    "np.random.seed(42)\n",
    "\n",
    "random_indexes = np.random.choice(np.arange(df.shape[0]),\n",
    "                                  size=1000,\n",
    "                                  replace=False)\n",
    "\n",
    "df = df.iloc[random_indexes]\n",
    "print(f'New dataset size: {df.shape}')\n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7RNPP1N9FxD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2 Building the Co-occurence Matrix\n",
    "\n",
    "For each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1UknGoYvIBBA7ytkSlqm1NhF_lHt0iOwT)\n",
    "\n",
    "Let's define the simplest version of a **co-occurrence matrix** based on word counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbuS9RBprp3d",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Small dataset case\n",
    "You should have a vocabulary size that we can afford in terms of memory demand.\n",
    "\n",
    "$\\rightarrow$ You can easily instantiate the co-occurrence matrix and populate it iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phOfKV_8rp3d",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Large dataset case\n",
    "We have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts.\n",
    "\n",
    "$\\rightarrow$ The [Scipy package](https://docs.scipy.org/doc/scipy/reference/sparse.html) allows us to easily define sparse matrices that can be converted ot numpy arrays.\n",
    "\n",
    "**Suggestion**: The simplest way to build the co-occurrence matrix is via an incremental approach:\n",
    "1. Loop through dataset sentences\n",
    "2. Split into words\n",
    "3. Count co-occurrences within the given window frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers [$\\texttt{lil_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix) sparse format that is suitable to this case. \n",
    "\n",
    "You can check out other sparse formats, such as [$\\texttt{csr_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), and the corresponding building methods.\n",
    "\n",
    "Working with $\\texttt{lil_matrix}$ might take $\\sim 1h$ of time to build the whole dataset co-occurrence matrix. It is also possibile to work with $\\texttt{csr_matrix}$ but the approach is more complex (check the last example of the corresponding documentation page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0g9enjW9IuZ",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gc\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def co_occurrence_count(df: pd.DataFrame,\n",
    "                        idx_to_word: Dict[int, str],\n",
    "                        word_to_idx: Dict[str, int],\n",
    "                        window_size: int = 4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds word-word co-occurrence matrix based on word counts.\n",
    "\n",
    "    :param df: pre-processed dataset (pandas.DataFrame)\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "\n",
    "    :return\n",
    "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
    "    \"\"\"\n",
    "    vocab_size = len(idx_to_word)\n",
    "    co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for sentence in tqdm(df.text.values):\n",
    "        tokens = sentence.split()\n",
    "        for pos, token in enumerate(tokens):\n",
    "            start = max(0, pos - window_size)\n",
    "            end = min(pos + window_size + 1, len(tokens))\n",
    "\n",
    "            first_word_index = word_to_idx[token]\n",
    "\n",
    "            for pos2 in range(start, end):\n",
    "                if pos2 != pos:\n",
    "                    second_token = tokens[pos2]\n",
    "                    second_word_index = word_to_idx[second_token]\n",
    "                    co_occurrence_matrix[first_word_index, second_word_index] += 1\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dkzkF2Rrp3d",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Clean RAM before re-running this code snippet to avoid possible session crash\n",
    "if 'co_occurrence_matrix' in globals():\n",
    "    del co_occurrence_matrix\n",
    "    gc.collect()\n",
    "    time.sleep(10.)     # Give colab some time \n",
    "\n",
    "print(\"Building co-occurrence count matrix... (it may take a while...)\")\n",
    "co_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4)\n",
    "print(\"Building completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDXIClMH9kcO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3 Embedding Visualization\n",
    "\n",
    "The next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdh0-RJQrp3e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**How?** We will explore SVD, t-SNE methods, and Umap, without delving into technical details since they are not arguments of this NLP course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-zCGK6-rp3e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**SVD Memo**: SVD stands for **Singular Value Decomposition** and is a kind of generalized **Principal Components Analysis** (PCA) and focuses on selecting the top **k** principal components. For more info, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) you can find a brief tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM91KE6qrp3e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**t-SNE Memo**: t-SNE stands for **t-Distributed Stochastic Neighbour Embedding** and is an unsupervised non-linear technique.\n",
    "* It preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. \n",
    "* The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. \n",
    "* Properly using t-SNE is a bit tricky, a well recommended reading is one of the [author's blog](https://lvdmaaten.github.io/tsne/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JFFdOberp3e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**UMAP Memo**: UMAP stands for **Uniform Manifold Approximation and Projection for Dimensionality Reduction** and is an unsupervised non-linear technique like t-SNE.\n",
    "\n",
    "* **Faster** than t-SNE\n",
    "* **More accurate** than t-SNE in terms of data's global structure preservation\n",
    "* Informally constructs a high-dimensional graph representation of the data and then optimizes a low-dimensional graph to be as structurally similar as possibile\n",
    "* Check this [blog](https://pair-code.github.io/understanding-umap/) if you are interested!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnAZSVoIrp3e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Play with the **window size** and check if there are some notable differences. <br>\n",
    "\n",
    "Generally:\n",
    "* Small window size $\\rightarrow$ reflects syntactic properties.\n",
    "* Large window size $\\rightarrow$ captures semantic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVFSLAiW4a64",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z38cC84Krp3e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6gpCg8n9mw7",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations: List[str] = None,\n",
    "                         word_to_idx: Dict[str, int] = None):\n",
    "    \"\"\"\n",
    "    Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "    :param word_annotations: list of words to be annotated.\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "    if word_annotations:\n",
    "        print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "        word_indexes = []\n",
    "        for word in word_annotations:\n",
    "            word_index = word_to_idx[word]\n",
    "            word_indexes.append(word_index)\n",
    "\n",
    "        word_indexes = np.array(word_indexes)\n",
    "\n",
    "        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "        target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "        for word, word_index in zip(word_annotations, word_indexes):\n",
    "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "            ax.annotate(word, xy=(word_x, word_y))\n",
    "    else:\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "    # We avoid outliers ruining the visualization if they are quite far away\n",
    "    axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n",
    "    axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n",
    "    plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n",
    "    plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n",
    "    ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n",
    "    ax.set_ylim(axis_y_limit[0], axis_y_limit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYWb_U-Drp3e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies SVD dimensionality reduction.\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                       of a word-word co-occurrence matrix the matrix shape would\n",
    "                       be (words, words).\n",
    "\n",
    "    :return\n",
    "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "    \"\"\"\n",
    "    print(\"Running SVD reduction method...\")\n",
    "    svd = TruncatedSVD(n_components=2, n_iter=10)\n",
    "    reduced = svd.fit_transform(embeddings)\n",
    "    print(\"SVD reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies t-SNE dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "    tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n",
    "    reduced = tsne.fit_transform(embeddings)\n",
    "    print(\"t-SNE reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies UMAP dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "    umap_emb = umap.UMAP(n_components=2, metric='cosine')\n",
    "    reduced = umap_emb.fit_transform(embeddings)\n",
    "    print(\"UMAP reduction completed!\")\n",
    "    \n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rASm9X4rp3e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Feel free to play with word_annotations argument!\n",
    "# Check the saved dictionary!\n",
    "def run_visualization(method_name: str,\n",
    "                      words_list: List[str],\n",
    "                      word_to_idx: Dict[str, int],\n",
    "                      co_occurrence_matrix):\n",
    "    method_name = method_name.lower().strip()\n",
    "    method_map = {\n",
    "        'svd': reduce_SVD,\n",
    "        'tsne': reduce_tSNE,\n",
    "        'umap': reduce_umap\n",
    "    }\n",
    "    \n",
    "    if method_name not in method_map:\n",
    "        raise RuntimeError(f'Invalid method name! Got {method_name}.')\n",
    "    \n",
    "    reduced = method_map[method_name](co_occurrence_matrix)\n",
    "    visualize_embeddings(reduced, words_list, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 'umap', 'tsne', 'svd'\n",
    "run_visualization('umap', ['good', 'love', 'beautiful'], word_to_idx, co_occurrence_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Play with different dimension reduction techniques.\n",
    "\n",
    "Note that these techniques are very sensitive to some of their hyper-parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8R1DwJg9-L9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4 Embedding properties\n",
    "\n",
    "Visualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space.\n",
    "\n",
    "$\\rightarrow$ For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReeTvqkCrp3e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### How to do that?\n",
    "We could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together.\n",
    "\n",
    "**However**, this method is rather **inaccurate** and **time-consuming** (dimensionality reduction is not a perfect mapping).\n",
    "\n",
    "$\\rightarrow$ We need some sort of similarity metric that is independent of the vector dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7e6iVYA-BZk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5 Cosine Similarity\n",
    "\n",
    "We want to measure how two word vectors are far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DGpClSWrp3f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Naive Method\n",
    "\n",
    "A naive solution would involve computing the dot product of the two vectors. \n",
    "\n",
    "However, this metric will give higher similarity either to longer vectors or to vectors that have higher counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQEMlf5-rp3f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "A better metric is **cosine similarity** which is just a normalized dot product.\n",
    "\n",
    "\\begin{align}\n",
    "s(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}\n",
    "\\end{align}\n",
    "\n",
    "where $s(p, q) \\in [-1, 1] $, since it computes the cosine of the angle between the two vectors. \n",
    "\n",
    "Intuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eXzwsPw-Fko",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as sk_cs\n",
    "\n",
    "def cosine_similarity(p: np.ndarray,\n",
    "                      q: np.ndarray,\n",
    "                      transpose_p: bool = False,\n",
    "                      transpose_q: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity of two d-dimensional matrices\n",
    "\n",
    "    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n",
    "    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n",
    "    :param transpose_p: whether to transpose p or not\n",
    "    :param transpose_q: whether to transpose q or not\n",
    "\n",
    "    :return\n",
    "        - cosine similarity matrix S of shape (p_samples, q_samples)\n",
    "          where S[i, j] = s(p[i], q[j])\n",
    "    \"\"\"\n",
    "    if len(p.shape) == 1:\n",
    "        p = p.reshape(1, -1)\n",
    "    if len(q.shape) == 1:\n",
    "        q = q.reshape(1, -1)\n",
    "\n",
    "    return sk_cs(p.T if transpose_p else p, q.T if transpose_q else q)\n",
    "\n",
    "similarity_matrix = cosine_similarity(co_occurrence_matrix,\n",
    "                                      co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd1Itvn5-h12",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.6 [Let's play] Analogies, Bias, Synonyms and Antonyms\n",
    "\n",
    "Let's look for some words and provide a possible explanation of achieved results according to cosine similarity metric.\n",
    "\n",
    "* Synonym pair: (w1, w2) such that w1 and w2 are synonyms\n",
    "* Antonyms pair: (w1, w2) such that w1 and w2 are antonyms\n",
    "* Synonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ti71XDdrp3f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Analogy\n",
    "\n",
    "Another useful property to check is analogy resolution via word vectors. \n",
    "\n",
    "In particular, we might want to check if analogies such \"man : king == woman : x\" bring results like \"x = queen\".\n",
    "\n",
    "In order to do so, we first need to define a ranking function that returns the top $K$ most similar words of a given word vector.\n",
    "\n",
    "$\\rightarrow$ We might not want to be too much restrictive and play with $K \\ge 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV4ZW-zS4WFK",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def show_similarity_for_nary(words: List[str],\n",
    "                             similarity_matrix: np.ndarray,\n",
    "                             word_to_idx: Dict[str, int],\n",
    "                             idx_to_word: Dict[int, str]):\n",
    "    \"\"\"\n",
    "    Shows similarity values for each pair of input words.\n",
    "\n",
    "    :param words: a list of candidate words.\n",
    "    :param similarity_matrix: np.ndarray containing similarity values \n",
    "    between each vocabulary word pair\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "\n",
    "    \"\"\"\n",
    "    word_indexes = [word_to_idx[word] for word in words]\n",
    "    similarity_dict = {}\n",
    "\n",
    "    for comb in product(word_indexes, word_indexes):\n",
    "        similarity_value = similarity_matrix[comb[0], comb[1]]\n",
    "        similarity_dict.setdefault(comb[0], []).append(similarity_value)\n",
    "\n",
    "    similarity_df = pd.DataFrame.from_dict(similarity_dict)\n",
    "    similarity_df.columns = [idx_to_word[col] for col in similarity_df.columns]\n",
    "    similarity_df.index = similarity_df.columns\n",
    "    similarity_df = similarity_df.transpose()\n",
    "    print(F'Similarity values: \\n{similarity_df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show_similarity_for_nary(['film', 'movie'], similarity_matrix, word_to_idx, idx_to_word)\n",
    "show_similarity_for_nary(['good', 'bad'], similarity_matrix, word_to_idx, idx_to_word)\n",
    "show_similarity_for_nary(['good', 'well', 'bad'], similarity_matrix, word_to_idx, idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzVmI7eR-lak",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_K_indexes(data: np.ndarray, K: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the top K indexes of a 1-dimensional array (descending order)\n",
    "    Example:\n",
    "        data = [0, 7, 2, 1]\n",
    "        best_indexes:\n",
    "        K = 1 -> [1] (data[1] = 7)\n",
    "        K = 2 -> [1, 2]\n",
    "        K = 3 -> [1, 2, 3]\n",
    "        K = 4 -> [1, 2, 3, 4]\n",
    "\n",
    "    :param data: 1-d dimensional array\n",
    "    :param K: number of highest value elements to consider\n",
    "\n",
    "    :return\n",
    "        - array of indexes corresponding to elements of highest value\n",
    "    \"\"\"\n",
    "    best_indexes = np.argsort(data, axis=0)[::-1]\n",
    "    best_indexes = best_indexes[:K]\n",
    "\n",
    "    return best_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcCFE_BCrp3f",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_K_word_ranking(embedding_matrix: np.ndarray, idx_to_word: Dict[int, str],\n",
    "                           word_to_idx: Dict[str, int],  positive_listing: List[str],\n",
    "                           negative_listing: List[str],  K: int) -> (List[str], np.ndarray):\n",
    "    \"\"\"\n",
    "    Finds the top K most similar words following this reasoning:\n",
    "        1. words that have highest similarity to words in positive_listing\n",
    "        2. words that have highest distance to words in negative_listing\n",
    "        \n",
    "    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param positive_listing: list of words that should have high similarity with top K retrieved ones.\n",
    "    :param negative_listing: list of words that should have high distance to top K retrieved ones.\n",
    "    :param K: number of best word matches to consider\n",
    "\n",
    "    :return\n",
    "        - top K word matches according to aforementioned criterium\n",
    "        - similarity values of top K word matches according to aforementioned criterium\n",
    "    \"\"\"\n",
    "    # Positive words (similarity)\n",
    "    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n",
    "    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n",
    "\n",
    "    # Negative words (distance)\n",
    "    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n",
    "    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n",
    "\n",
    "    # Find candidate words\n",
    "    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n",
    "    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n",
    "    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n",
    "    candidate_vectors = embedding_matrix[valid_indexes]\n",
    "\n",
    "    candidate_similarities = cosine_similarity(candidate_vectors, target_vector)\n",
    "    candidate_similarities = candidate_similarities.ravel()\n",
    "\n",
    "    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n",
    "    top_K_indexes = valid_indexes[relative_indexes]\n",
    "    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n",
    "\n",
    "    return top_K_words, candidate_similarities[relative_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui0Qbn2L-rM8",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Positive and negative listing can be defined accordingly to a given analogy\n",
    "    Example:\n",
    "        \n",
    "        man : king :: woman : x\n",
    "    \n",
    "    positive_listing = ['king', 'woman']\n",
    "    negative_listing = ['man']\n",
    "\n",
    "    This is equivalent to: compute king - man + woman, and then find the\n",
    "    most similar candidate.\n",
    "\"\"\"\n",
    "\n",
    "# Examples\n",
    "# tv : episodes :: film : x\n",
    "# masterpiece : superb :: x : tragic\n",
    "top_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=co_occurrence_matrix,\n",
    "                                                   idx_to_word=idx_to_word,\n",
    "                                                   word_to_idx=word_to_idx,\n",
    "                                                   positive_listing=['episodes', 'film'],\n",
    "                                                   negative_listing=['tv'],\n",
    "                                                   K=10)\n",
    "print(f'Top K words: {top_K_words}')\n",
    "print(f'Top K values: {top_K_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Take some time to play with different analogies.\n",
    "\n",
    "**How?** Check the vocabulary and related tokens frequency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpohtztJrp3f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART II\n",
    "\n",
    "*   Loading pre-trained **dense** word embeddings: Word2Vec, GloVe, FastText.\n",
    "*   Checking **out-of-vocabulary** (OOV) terms.\n",
    "*   **Handling** OOV terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKaAlvqhFaSE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Dense embeddings\n",
    "\n",
    "Until now we've worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to $|V|$). \n",
    "\n",
    "The **main drawback** of such approach is that words belong to separate dimensions.\n",
    "\n",
    "$\\rightarrow$ We need to have a large corpus available to check if two words have similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z3z2u6orp3g",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dense Embedding Technique\n",
    "\n",
    "To this end, we might prefer a dense embedding technique. <br>\n",
    "$\\rightarrow$ All words are encoded into a high dimensional space, much smaller than $|V|$ (generally up to $\\sim 1000$).\n",
    "\n",
    "A dense representation is also convenient from a machine learning point of view:\n",
    "*    **Fewer parameters** to learn and, thus, models are less prone to overfitting.\n",
    "*    Words do not belong to separate dimensions anymore and semantic relationships are easily modelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MpHvK4FFeap",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Working with a pre-trained model\n",
    "\n",
    "The first step consists in choosing and downloading a pre-trained embedding model. \n",
    "\n",
    "For the purpose of this assignment, we limit to classic models, such as Word2Vec, GloVe and FastText.\n",
    "\n",
    "Furthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size.\n",
    "\n",
    "$\\rightarrow$ We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3mQB6NNrp3g",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Brief Recap\n",
    "\n",
    "For a full list of available embeddings models, please check [here](https://github.com/RaRe-Technologies/gensim-data).\n",
    "\n",
    "* **Word2Vec**: the first example of dense word encoding. There are two well-known strategies:\n",
    "     * Continuous Bag-of-words (CBoW): context words are used to predict a target word.\n",
    "     * Skip-gram: a target word is used to predict its own context.\n",
    "* **GloVe**: it is another techniques that tries to encoded global semantic properties based on the co-occurrence matrix. Conversely, word2vec exploits local information.\n",
    "* **Fasttext**: an extension of word2vec where words are defined as character n-grams. It works very well with rare words contrarily to Word2Vec and GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMd7o1cO4fxo",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIoWFwyAFmIg",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        print('FastText: 300')\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUtONZmDrp3g",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Modify these variables as you wish!\n",
    "# Glove -> 50, 100, 200, 300\n",
    "# Word2Vec -> 300\n",
    "# Fasttext -> 300\n",
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Play with different embedding models and embedding sizes.\n",
    "\n",
    "Compare the achieved visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCsxb5g6Fnc_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Out of vocabulary (OOV) words\n",
    "\n",
    "Before evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset.\n",
    "\n",
    "$\\rightarrow$ We check the number of out-of-vocabulary (OOV) terms.\n",
    "\n",
    "If the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi1Rq5cBrp3g",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Which one?** One common practice is to assign a **random vector**, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process. \n",
    "\n",
    "Even if that is the case, we can assign an embedding that is more meaningful rather than a random one.\n",
    "\n",
    "$\\rightarrow$ We can identify the word embedding of an OOV term as the **mean of its neighbour word embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8s86gnQmFp-q",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpZzHiNMrp3g",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrCrkTdiFsYp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Handling OOV words\n",
    "\n",
    "Now we proceed on building the embedding matrix, while handling OOV terms at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks3PUZeyFuYB",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int],\n",
    "                           vocab_size: int,\n",
    "                           oov_terms: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIheFWL0rp3g",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "embedding_dimension = 50\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT21zEwuFwbJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4 Embedding visualization (cont'd)\n",
    "\n",
    "We are now ready to visualize pre-trained word embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU0xDlLfFzMc",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# UMAP\n",
    "reduced_embedding_umap = reduce_umap(embedding_matrix)\n",
    "visualize_embeddings(reduced_embedding_umap, ['good', 'love', 'beautiful'], word_to_idx)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gay1jSfwF2Qf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.5 [Let's play!] Embedding properties (cont'd)\n",
    "\n",
    "Let's consider some previous examples for quick comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yMbNuIdF5s0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "top_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=embedding_matrix,\n",
    "                                                   idx_to_word=idx_to_word,\n",
    "                                                   word_to_idx=word_to_idx,\n",
    "                                                   positive_listing=['episodes', 'film'],\n",
    "                                                   negative_listing=['tv'],\n",
    "                                                   K=10)\n",
    "print(f'Top K words: {top_K_words}')\n",
    "print(f'Top K values: {top_K_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Compare word embedding analysis between sparse and dense word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Takeway Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXd-EA6orp3l",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Teaching \n",
    "\n",
    "Your TA should be quite thirsty and tired at this point, make sure they are okay even though you didn't understand a single word of their \"English\". Possibly, offer them a beer 💞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRge6E5Drp3m",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dense vs Sparse Embeddings\n",
    "\n",
    "Dense word embeddings are very cool w.r.t. to sparse ones. However, we've just explored fixed embedding models: each word has its own embedding vector independently of the current context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EyH7Hwprp3m",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Out-of-vocabulary Terms\n",
    "OOV terms are everywhere, when we should worry about them? Have a look at pre-trained embedding models if they are suitable to your domain to drastically reduce this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbzMCMfprp3m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "-5qeqzylrp3b",
    "GoSJR73EF2Tf",
    "F7RNPP1N9FxD",
    "tbuS9RBprp3d",
    "1DGpClSWrp3f",
    "3Z3z2u6orp3g",
    "3MpHvK4FFeap",
    "8skS3zDirp3h",
    "ORdyyfTFKQLL",
    "uw_t8aJ1rp3i",
    "vsSz83VK5w57",
    "qJVARaz863mV",
    "gDvYcP0jrp3j",
    "5wdOk5Rjrp3j",
    "VjxXIQuWrp3k",
    "5iuOM6hh9nq7",
    "fes2Uu03rp3k",
    "1efSCTI8rp3l",
    "kkD7ZpXvrp3l",
    "ZXd-EA6orp3l",
    "lRge6E5Drp3m",
    "2EyH7Hwprp3m",
    "ekJRVODarp3m",
    "sJJeSx7Hrp3m",
    "Ilhj-bbirp3m",
    "b-VuPx8Drp3m"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
